{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Perception: Learning Event-based Data and Motion Compensation\n",
    "## Event-based Cameras\n",
    "Event-based Cameras (EBCs) are imaging sensors that respond to local brightness changes. Unlike conventional frame-based cameras, EBCs are asynchronous, having each pixel operating independently from the other. Each pixel reports a change of intensity (if it occurs), or stays silent otherwise. For example, a pixel stores a reference intensity at time $t$, the current intensity is then compared to the reference at time $t + 1$. If the aforementioned difference exceeds the threshold, that pixel outputs an **event**.\n",
    "\n",
    "An **event** is a discrete packet of information containing:\n",
    "* Event location ($x$ and $y$ coordinates, across the height and width dimensions respectively).\n",
    "* Event timestamps $t$, the time at which the event occurred.\n",
    "* Event polarity $p$, the direction of intensity change. It does not contain any information regarding intensity values. It is an output spike that can either be $+1$, or $-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure\n",
    "### Dataset\n",
    "For the sake of this lab, we will be using the **MVSEC** dataset. The MVSEC dataset is designed for the development of novel 3D perception algorithms for EBCs. In fact, stereo event-based data is collected from cars, motorbikes, hexa-copters and handhelds, fused with LiDAR, IMU, motion-capture and GPS systems to provide ground truth pose and depth images.\n",
    "\n",
    "> **Note:**\n",
    "> You can read more about the \"MVSEC\" dataset [here](https://daniilidis-group.github.io/mvsec/). Before starting this lab, it is important to understand the data structure of MVSEC. This dataset has been taken with a **DAVIS346B** sensor of resolution **(260, 346)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eWiz\n",
    "To load and manipulate the dataset, we will be using our own **eWiz** library for event-based data reading and manipulation. The **eWiz** library contains a series of modules, which include, but is not limited to:\n",
    "* **Data Reading:** Reads and clips event-based and ground truth flow optical flow data, which use the eWiz format.\n",
    "* **Data Rendering:** Renders event-based sequences.\n",
    "* **Data Augmentations:** Applies temporal and spatial augmentations to event-based data and corresponding grayscale images.\n",
    "* **Accuracy Metrics:** Provides several accuracy metrics for optical flow evaluation, such as the *Average Endpoint Error (AEE)*, and the percentage of outliers.\n",
    "* **Visualizations:** Provides visualization functions for optical flow, event-based data, and grayscale images.\n",
    "\n",
    "### Installation\n",
    "First, install eWiz by following the steps on the official repository [here](https://github.com/CIRS-Girona/ewiz).\n",
    "You can check the official [documentation](https://ewiz.readthedocs.io) to see how to use the functions in this lab (API References section). Also, download the *Indoor Flying 1* dataset of MVSEC in the eWiz format [here](https://drive.google.com/drive/folders/1orzPdKZk1NNHhkuW2iA3bTpb11d5JSzg?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1:** Data Loading\n",
    "In this part, you will be looking at the full sequence of one of the datasets. Your tasks are:\n",
    "1. Choose the dataset, and render it using the `VideoRendererBase` class.\n",
    "2. Choose a sequence of *0.2 s*, in which you observe a clear movement in one direction. Specify in your answer the chosen timestamps.\n",
    "3. Extract that sequence using the data reader of eWiz, and print the extracted events.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Renderer\n",
    "Write the data renderer code in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ewiz.renderers.videos import VideoRendererBase\n",
    "\n",
    "# ===== Run video renderer below ===== #\n",
    "# Do not forget to look at the documentation to understand how to run the renderer\n",
    "# [Code here.] #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reading\n",
    "Clip the sequence in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ewiz.data.readers import ReaderBase\n",
    "\n",
    "# ===== Clip data here ===== #\n",
    "# Do not forget to look at the documentation to understand how to run the renderer\n",
    "# [Code here.] #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ewiz.renderers.videos import VideoRendererBase\n",
    "\n",
    "# ===== Video renderer check ===== #\n",
    "# Double-check if this is the sequence you want with the video renderer\n",
    "# [Code here.] #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "1. What could cause an event to have a positive or negative polarity?\n",
    "2. Given the timestamp resolution, calculate the maximum theoretical event rate this sensor can handle. How would this impact applications like high-speed object tracking? Show explanation.\n",
    "3. Event-based sensors can produce noise events (false positives) due to thermal noise or flickering light sources. Based on the initial event samples, how could you identify and filter out noise without significantly impacting true events? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **[Answer Here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2:** Data Manipulation\n",
    "Event-based data is **asynchronous**, meaning that unlike intensity frame-based cameras, EBCs do not output data at a fixed frequency. This might be a problem for traditional neural network architectures (or computer vision algorithms) as they require data given synchronously. Moreover, EBCs are considered a paradigm shift as they do not output traditional images. As such, if we want to work with convolutional neural networks (CNNs) this event-based data (the array you printed previously) should be converted to an **image-like** equivalent. This is what we call events-to-image encoding, which converts event-based data to a traditional image (either grayscale, RGB, or 2-channels).\n",
    "\n",
    "For the sake of this lab, let's start by creating a simple events image. Using the **events** data you printed earlier, we want to insert each event in its respective pixel location, for which each channel corresponds to a different polarity. As such, the output image will have the shape **(C, H, W)**, where:\n",
    "* **C:** The channel dimension (its value is 2), the first element encapsulates **positive** polarities, whereas the second element encapsulates **negative** polarities.\n",
    "* **H:** The **height** dimension, which corresponds to the y-dimension of the sensor.\n",
    "* **W:** The **width** dimension, which corresponds to the x-dimension of the sensor.\n",
    "\n",
    "> **Note:** Each pixel value should correspond to the number of events that took place at that pixel location (sum of events, or **count** method).\n",
    "\n",
    "Your job is to:\n",
    "* Convert extracted events to an image representation, fill the `convert_events_to_image` function.\n",
    "* Use the provided visualization functions to visualize the images:\n",
    "    * A **heatmap** representation of the combined channels, printing the pixel value with the highest number of events.\n",
    "    * An RGB image showing **positive alone**, **negative alone**, and **combined** events representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "# ===== Code inside function ===== #\n",
    "def convert_events_to_image(\n",
    "    events: np.ndarray, image_size: Tuple[int, int]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts a sequence of events to a 2-channel events image.\n",
    "\n",
    "    Args:\n",
    "        events (np.ndarray): Events array, returned by the data reader.\n",
    "        image_size (Tuple[int, int]): The size of the output image, equal to\n",
    "            the size of the sensor of format (H, W).\n",
    "\n",
    "    Returns:\n",
    "        events_image (np.ndarray): Output image of shape (2, H, W). Each pixel\n",
    "            should contain an integer with the number of events that occurred at that location.\n",
    "    \"\"\"\n",
    "    # [Code here.] #\n",
    "\n",
    "\n",
    "# ================================ #\n",
    "\n",
    "\n",
    "def visualize_events_heatmap(\n",
    "    events_image: np.ndarray, polarity: str = \"pos\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Visualizes events heatmap based on the pixel-wise count of events.\n",
    "\n",
    "    Args:\n",
    "        events_image (np.ndarray): Generated events image.\n",
    "        polarity (str): Polarity channel to show, can either be \"pos\", \"neg\", \"both\". Defaults to \"pos\".\n",
    "\n",
    "    Returns:\n",
    "        image (np.ndarray): Output image of shape (H, W).\n",
    "    \"\"\"\n",
    "    if polarity == \"pos\":\n",
    "        image = events_image[0, ...]\n",
    "    elif polarity == \"neg\":\n",
    "        image = events_image[1, ...]\n",
    "    else:\n",
    "        image = np.sum(events_image, axis=0)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    return image\n",
    "\n",
    "\n",
    "def visualize_events_image_rgb(\n",
    "    events_image: np.ndarray, polarity: str = \"pos\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Visualizes events heatmap based on the pixel-wise count of events.\n",
    "\n",
    "    Args:\n",
    "        events_image (np.ndarray): Generated events image.\n",
    "        polarity (str): Polarity channel to show, can either be \"pos\", \"neg\", \"both\". Defaults to \"pos\".\n",
    "\n",
    "    Returns:\n",
    "        image (np.ndarray): Output image of shape (H, W, 3).\n",
    "    \"\"\"\n",
    "    image_size = (events_image.shape[1], events_image.shape[2])\n",
    "    image = np.zeros(((3,) + image_size), dtype=np.uint8) + 255\n",
    "    if polarity == \"pos\":\n",
    "        pos_ids = np.where(events_image[0, ...] > 0)\n",
    "        image[:, pos_ids[0], pos_ids[1]] = np.array([255, 0, 0])[:, None]\n",
    "    elif polarity == \"neg\":\n",
    "        neg_ids = np.where(events_image[1, ...] > 0)\n",
    "        image[:, neg_ids[0], neg_ids[1]] = np.array([0, 0, 255])[:, None]\n",
    "    else:\n",
    "        pos_ids = np.where(events_image[0, ...] > 0)\n",
    "        neg_ids = np.where(events_image[1, ...] > 0)\n",
    "\n",
    "        image[:, pos_ids[0], pos_ids[1]] = np.array([255, 0, 0])[:, None]\n",
    "        image[:, neg_ids[0], neg_ids[1]] = np.array([0, 0, 255])[:, None]\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    return image\n",
    "\n",
    "\n",
    "# ===================== Code task here ===================== #\n",
    "# [Code here.] #\n",
    "# ========================================================== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "1. How does the choice of time window affect the visibility of objects? Explain.\n",
    "2. Would this representation work well for detecting fast-moving objects? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **[Answer Here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3:** Frame-based Approach\n",
    "There exists multiple encoding schemes that have been used in the literature for the sake of implementing event-based neural networks and algorithms. The goal behind such encoding schemes is to discretize the events volume, meaning that we want to create a sequence of events image frames before feeding them to the network.\n",
    "\n",
    "Unlike the previously generated image (which encapsulates all of the clipped sequence), we want to create 5 events frames for your clipped sequence:\n",
    "* Generate an array containing all frames of shape **(C, H, W, N)**, where N is the frame channel.\n",
    "* Visualize combined **RGB** images frame by frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Code task here ===================== #\n",
    "# [Code here.] #\n",
    "# ========================================================== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "> **Note**: Extend the chosen sequence to 1 s, you will be comparing the images you got for a slice of 0.2 s, and the same slice extended to 1.0 s. Answer all questions below in your report for each case (0.2 s and 1.0 s):\n",
    "\n",
    "1. How clear is the obtained events image? Why? (Explain by showing your resulting images, and analyzing the motion of your chosen sequence).\n",
    "2. How do the images change after applying the frame-based approach? Why? (Explain by showing your resulting images, and analyzing the motion of your chosen sequence).\n",
    "\n",
    "One answer only for both cases:\n",
    "1. For the same time duration, how would the clarity of the events image change if we increase the camera velocity for the same scene? Explain.\n",
    "2. Which event representation do you find most informative? The one-image, or frame-based approach? Why?\n",
    "3. In this lab, we only used event counts as encoding. Though this might seem a good idea, some information remains lost. Pinpoint which information is lost with such an encoding scheme and suggest alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **[Answer Here]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ewiz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
